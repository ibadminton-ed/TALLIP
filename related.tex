\section{Related Work}\label{related}
Event extraction is one of the challenging tasks in information extraction. Many approaches have been explored from different views, which can be divided into  traditional feature-based methods and neural-network-based methods.

Feature-based methods exploit various feature extraction strategies and evaluate feature contributions for prediction. \citeN{ahn2006stages} models the event extraction task as a pipeline of four classification tasks and applies lexical features (e.g., full word, word lemma, etc.), syntactic features (e.g., dependency trees, SRL structures, etc.), and  features generated from external knowledge resources (e.g., WordNet). Later studies also employ context  sentences and higher level information to characterize global features. For example, \citeN{ji2008refining} utilize global evidence from a cluster of topically-related documents to break down the document boundaries for event extraction. And \citeN{patwardhan2009unified} consider both local context and a wider sentential context around a phrase  to estimate whether a sentence is discussing an event of interest. \citeN{gupta2009predicting} employ cross-event features to extract implicit time information. \citeN{liao2010using} explore document level cross-event information to resolve ambiguities between certain types of events. \citeN{hong2011using} leverage cross-entity inference to predict event mentions.  \citeN{huang2012modeling}  propose a bottom-up approach that initially identifies candidate argument independently and then uses that information to model textual cohesion. More recently, some researchers have tried to improve other aspects of event extractions. As opposed to traditional pipelined approaches, \citeN{li2013joint} attempt to jointly learn all ACE event extraction subtasks using structured perceptron and incorporate global features which explicitly capture the dependencies of multiple triggers and arguments in a transition system.

Recently, neural network models have been employed to achieved competitive performances against traditional models in many NLP tasks. \citeN{chen2015event} propose a convolutional neural network (CNN) to capture lexical-level clues, with a dynamic multi-pooling layer to capture sentence-level features. Recurrent neural network (RNN) is also a popular and effective choice for classification tasks. \citeN{ghaeini2016event} effort to detect triggers that can be either words or phrases by a forward-backward RNN. Nguyen et al. [2016] propose a bidirectional RNN with various memory matrices to jointly learn triggers and arguments, which benefits from the advantages of both joint models and neural network models. \citeN{feng2016language} introduce a language-independent hybrid neural network model that incorporates both bidirectional LSTMs and convolutional neural networks, which yields competitive performances on both English and Chinese event extraction.

Compared to the amount of work on English event extraction, there are a few studies focusing on Chinese event extraction, and most of them are feature-based methods. \citeN{chen2009language} are the first to report the language specific issues in Chinese trigger labeling, and apply various kinds of lexical, syntactic and semantic features to the modularized pipeline system.  One of their simple yet effective strategies is to build an errata table to deal with the inconsistency between word segmentation results and trigger annotations. The table includes the correspondence between words and their frequent labeling results, e.g., the word ``打死'' is often labeled as ``打''$-$\emph{Attack} and ``死''$-$\emph{Die}, and during testing, if the word ``打死''  has been identified as a trigger candidate, we will directly label each character with their specified event types in the errata table, respectively.  However, this strategy is incapable to handle the ``落入法网'' case, where ``落入'' and ``法网'' are usually treated as two words, since the candidate search space is vast. 
 \citeN{li2012employing} also explore language phenomena in Chinese event extraction, and alleviate the word segmentation errors via compositional semantics inside Chinese triggers and discourse consistency between Chinese triggers. The state-of-art feature-based model investigates two extensions to \citeN{li2012employing}, namely joint-learning method and knowledge-rich method. As for the first extension, they divide the original four-step pipeline system into two joint learning tasks where they jointly learn trigger identification and trigger classification, and then jointly learn argument identification and argument classification. In the second extension, they apply six groups of features motivated by deep investigations on the dataset: zero pronoun features and trigger type consistency features  extracted by well-designed rules,  syntactic and semantic features extracted using a Chinese semantic role labeling system, and also help from external linguistic resources, e.g.,  a Chinese synonym dictionary for character-based features. As we can see,  this method heavily relies on elaborate feature templates and external tools or thesauruses, thus may suffers from the errors propagated from other NLP tools.

In this paper, we follow the two-step joint-learning style that alleviates the error propagation problems of traditional pipeline systems, and propose two neural network models to avoid heavy feature engineering and extra resources. Specifically, the CNN component learns lexical features while the bidirectional LSTM component extracts sentence-level features automatically. We further combine a CRF layer to learn sequence labeling preferences to jointly decode the best labels. % \textbf{And these models are flexible that both two subtasks share the similar neural network architectures}.
To further deal with specific issues with respect to Chinese, we,  instead of labeling word by word, we introduce the character-based models to avoid the reliance on carefully designed resources for Chinese event extraction.
